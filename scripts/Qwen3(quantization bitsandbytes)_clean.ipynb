{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5PqhRzUQjmJ",
    "outputId": "0b11c645-2353-441b-cf10-f8f9d0522a20"
   },
   "outputs": [],
   "source": [
    "pip install torch transformers auto_gptq datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ud-uxtk7Qw1s",
    "outputId": "b29a450f-892a-450a-9f2b-f80093cebad4"
   },
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDXBtw-mQxD6",
    "outputId": "e1804cc5-ea78-4624-eda5-8b2e16467726"
   },
   "outputs": [],
   "source": [
    "pip install peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s5vUaUHQxOd",
    "outputId": "4483799f-a446-4e64-cfb2-f6b23cdbfa32"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ttkby6BCQxXC",
    "outputId": "a67dfc73-43bd-4d5e-fc55-d46c6a5c5035"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers peft auto-gptq accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-GRQmHH9zcb",
    "outputId": "37b4bfe2-d6a7-4307-ccfc-04f27894ca4b"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzZrDpYtRpyz"
   },
   "outputs": [],
   "source": [
    "!export HF_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWu7A4ngQxfw",
    "outputId": "45602d4c-3f2f-48cd-b3db-a2e69aec9efa"
   },
   "outputs": [],
   "source": [
    "%env HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676,
     "referenced_widgets": [
      "c7ca76c47dab47d199714d509b8aff3b",
      "dc772b550d63479daea13c1895767614",
      "596e4bafd2e94013a248d090fa49293b",
      "4e5c6cbde5594d70bd3fc8020481bed3",
      "a5af96f5414b46a1bd148829ca71ec4f",
      "946f842f8bbd4926b10baa75e207eb6c",
      "f067439c04b44402a405f3630850c808",
      "8627fe3a11e3435ca0e1bdf24711c560",
      "bd844bdbb3f24bae8043e61b968cf77e",
      "fd987f8a375e42abb26fcea1e9cf9a8e",
      "33e5e3d7d7ad48ee82f358cbf80de51f"
     ]
    },
    "id": "L2EAe8laQxok",
    "outputId": "5b95a15f-da30-4359-d171-65ab25d10c27"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model and token\n",
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Define prompts\n",
    "prompts = [\n",
    "    \"What is the capital of France, and what is its largest city?\",\n",
    "    \"Write a short story about a robot exploring an abandoned city.\",\n",
    "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
    "    \"Explain why the sky appears blue.\",\n",
    "    \"What’s your favorite book, and why?\"\n",
    "]\n",
    "\n",
    "def query_model(model, tokenizer, prompt, max_new_tokens=200, num_beams=2, temperature=0.7):\n",
    "    system_prompt = \"You are a helpful assistant. Provide a concise and accurate answer to the question without repeating the prompt or adding unrelated questions.\"\n",
    "    full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt}\\nAnswer:\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            early_stopping=False,\n",
    "            no_repeat_ngram_size=0\n",
    "        )\n",
    "    inference_time = time.perf_counter() - start_time\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[1].strip()\n",
    "    return response, inference_time\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model with 4-bit quantization\n",
    "    print(\"\\nLoading Qwen Model and Tokenizer with 4-bit quantization...\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "\n",
    "    # Measure memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        memory_usage = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "        print(f\"Model memory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    # Query model\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nPrompt {i}: {prompt}\")\n",
    "        response, inference_time = query_model(model, tokenizer, prompt)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "\n",
    "    print(\"\\nTesting complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE-mJJ4yVzNF"
   },
   "outputs": [],
   "source": [
    "### testing Qwen quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d19647c6abcb435da5c472cd928ca3ac",
      "5f5b78b7a4984ce5a95e5c2b0bfe6b37",
      "9e4e39c253044771b4097b33d6176303",
      "343bb0894e784d5d97eb68fe20a62026",
      "11100bb44d32489ebf900e3f26d1558a",
      "eac348febe7b412cb506f51bc41c2f4e",
      "4641313eb1a64ea680ecb5454a0d2a96",
      "aad6e3e89c5a42509454f72e1f61cb09",
      "7ee02e66932946d2ad9c25bf8ecb5640",
      "045cbb74136948bcaf2ecda8a6427d10",
      "f241641976f84d3f80d7ac62dfba73ff"
     ]
    },
    "id": "0T3JFH24QyPV",
    "outputId": "5dc47fef-f5a7-4ed6-c66e-ebccf82ac0cb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define model, token, and output directory\n",
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "HF_TOKEN = \"your_new_hf_token\"  # Replace with your new Hugging Face token\n",
    "output_dir = \"Qwen3-8B-Quantized\"\n",
    "\n",
    "# Define prompts for testing\n",
    "prompts = [\n",
    "    \"What is the capital of France, and what is its largest city?\",\n",
    "    \"Write a short story about a robot exploring an abandoned city.\",\n",
    "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
    "    \"Explain why the sky appears blue.\",\n",
    "    \"What’s your favorite book, and why?\",\n",
    "    # Additional prompts for robustness\n",
    "    \"Solve the equation: 2x + 5 = 15.\",\n",
    "    \"Describe the cultural significance of the Eiffel Tower.\",\n",
    "    \"Generate a Python function to reverse a string.\"\n",
    "]\n",
    "\n",
    "# Load evaluation dataset for perplexity\n",
    "def load_eval_dataset(num_examples=100):\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\", streaming=True)\n",
    "    texts = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= num_examples:\n",
    "            break\n",
    "        texts.append(example['text'][:512])  # Limit to 512 characters\n",
    "    return texts\n",
    "\n",
    "# Function to query model\n",
    "def query_model(model, tokenizer, prompt, max_new_tokens=200, num_beams=2, temperature=0.7):\n",
    "    system_prompt = \"You are a helpful assistant. Provide a concise and accurate answer to the question without repeating the prompt or adding unrelated questions.\"\n",
    "    full_prompt = f\"{system_prompt}\\n\\nQuestion: {prompt}\\nAnswer:\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            early_stopping=False,\n",
    "            no_repeat_ngram_size=0\n",
    "        )\n",
    "    inference_time = time.perf_counter() - start_time\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[1].strip()\n",
    "    return response, inference_time, len(outputs[0])\n",
    "\n",
    "# Function to compute perplexity\n",
    "def compute_perplexity(model, tokenizer, texts, batch_size=8):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "        total_loss += loss.item() * input_ids.size(1)\n",
    "        total_tokens += input_ids.size(1)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity\n",
    "\n",
    "# Function to test batch processing\n",
    "def test_batch_processing(model, tokenizer, prompts, batch_sizes=[1, 4, 8]):\n",
    "    results = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        start_time = time.perf_counter()\n",
    "        inputs = tokenizer(prompts[:batch_size], return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=200,\n",
    "                num_beams=2,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                early_stopping=False,\n",
    "                no_repeat_ngram_size=0\n",
    "            )\n",
    "        inference_time = time.perf_counter() - start_time\n",
    "        tokens_per_second = len(outputs[0]) * batch_size / inference_time\n",
    "        results[batch_size] = {\"time\": inference_time, \"throughput\": tokens_per_second}\n",
    "    return results\n",
    "\n",
    "try:\n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"\\nLoading Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load quantized model\n",
    "    print(\"\\nLoading Quantized Model (4-bit)...\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model_quantized = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    torch.cuda.synchronize()\n",
    "    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    print(f\"Quantized Model Memory Usage: {quantized_memory:.2f} MB\")\n",
    "\n",
    "    # Save quantized model and tokenizer\n",
    "    print(f\"\\nSaving Quantized Model to '{output_dir}'...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_quantized.save_pretrained(output_dir, safe_serialization=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Quantized model and tokenizer saved to '{output_dir}'.\")\n",
    "\n",
    "    # Load evaluation dataset\n",
    "    print(\"\\nLoading Evaluation Dataset...\")\n",
    "    eval_texts = load_eval_dataset()\n",
    "\n",
    "    # Test quantized model\n",
    "    results = {\n",
    "        \"quantized\": {\"times\": [], \"responses\": [], \"perplexity\": None, \"tokens\": []}\n",
    "    }\n",
    "\n",
    "    # Query quantized model\n",
    "    print(\"\\nTesting Quantized Model...\")\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        response, inference_time, tokens = query_model(model_quantized, tokenizer, prompt)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "        print(f\"Tokens Generated: {tokens}\")\n",
    "        results[\"quantized\"][\"times\"].append(inference_time)\n",
    "        results[\"quantized\"][\"responses\"].append(response)\n",
    "        results[\"quantized\"][\"tokens\"].append(tokens)\n",
    "\n",
    "    # Compute perplexity for quantized model\n",
    "    print(\"\\nComputing Perplexity for Quantized Model...\")\n",
    "    results[\"quantized\"][\"perplexity\"] = compute_perplexity(model_quantized, tokenizer, eval_texts)\n",
    "\n",
    "    # Test batch processing for quantized model\n",
    "    print(\"\\nTesting Batch Processing for Quantized Model...\")\n",
    "    quantized_batch_results = test_batch_processing(model_quantized, tokenizer, prompts)\n",
    "\n",
    "    # Summarize results\n",
    "    print(\"\\n=== Performance Summary ===\")\n",
    "    print(f\"Quantized Model Memory Usage: {quantized_memory:.2f} MB\")\n",
    "    print(f\"Average Inference Time: {np.mean(results['quantized']['times']):.4f} seconds\")\n",
    "    print(f\"Average Tokens Generated: {np.mean(results['quantized']['tokens']):.2f}\")\n",
    "    print(f\"Perplexity: {results['quantized']['perplexity']:.2f}\")\n",
    "    print(\"\\nBatch Processing Results:\")\n",
    "    for batch_size in quantized_batch_results:\n",
    "        print(f\"Batch Size {batch_size}:\")\n",
    "        print(f\"  Time: {quantized_batch_results[batch_size]['time']:.4f}s, Throughput: {quantized_batch_results[batch_size]['throughput']:.2f} tokens/s\")\n",
    "    print(\"\\nResponses:\")\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nPrompt {i}: {prompt}\")\n",
    "        print(f\"Response: {results['quantized']['responses'][i-1]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    if 'model_quantized' in locals():\n",
    "        del model_quantized\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6flp60OVZXu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "045cbb74136948bcaf2ecda8a6427d10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11100bb44d32489ebf900e3f26d1558a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33e5e3d7d7ad48ee82f358cbf80de51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "343bb0894e784d5d97eb68fe20a62026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_045cbb74136948bcaf2ecda8a6427d10",
      "placeholder": "​",
      "style": "IPY_MODEL_f241641976f84d3f80d7ac62dfba73ff",
      "value": " 5/5 [00:42&lt;00:00,  5.49s/it]"
     }
    },
    "4641313eb1a64ea680ecb5454a0d2a96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e5c6cbde5594d70bd3fc8020481bed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd987f8a375e42abb26fcea1e9cf9a8e",
      "placeholder": "​",
      "style": "IPY_MODEL_33e5e3d7d7ad48ee82f358cbf80de51f",
      "value": " 5/5 [00:25&lt;00:00,  4.20s/it]"
     }
    },
    "596e4bafd2e94013a248d090fa49293b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8627fe3a11e3435ca0e1bdf24711c560",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd844bdbb3f24bae8043e61b968cf77e",
      "value": 5
     }
    },
    "5f5b78b7a4984ce5a95e5c2b0bfe6b37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eac348febe7b412cb506f51bc41c2f4e",
      "placeholder": "​",
      "style": "IPY_MODEL_4641313eb1a64ea680ecb5454a0d2a96",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "7ee02e66932946d2ad9c25bf8ecb5640": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8627fe3a11e3435ca0e1bdf24711c560": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "946f842f8bbd4926b10baa75e207eb6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e4e39c253044771b4097b33d6176303": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aad6e3e89c5a42509454f72e1f61cb09",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ee02e66932946d2ad9c25bf8ecb5640",
      "value": 5
     }
    },
    "a5af96f5414b46a1bd148829ca71ec4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aad6e3e89c5a42509454f72e1f61cb09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd844bdbb3f24bae8043e61b968cf77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7ca76c47dab47d199714d509b8aff3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc772b550d63479daea13c1895767614",
       "IPY_MODEL_596e4bafd2e94013a248d090fa49293b",
       "IPY_MODEL_4e5c6cbde5594d70bd3fc8020481bed3"
      ],
      "layout": "IPY_MODEL_a5af96f5414b46a1bd148829ca71ec4f"
     }
    },
    "d19647c6abcb435da5c472cd928ca3ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f5b78b7a4984ce5a95e5c2b0bfe6b37",
       "IPY_MODEL_9e4e39c253044771b4097b33d6176303",
       "IPY_MODEL_343bb0894e784d5d97eb68fe20a62026"
      ],
      "layout": "IPY_MODEL_11100bb44d32489ebf900e3f26d1558a"
     }
    },
    "dc772b550d63479daea13c1895767614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_946f842f8bbd4926b10baa75e207eb6c",
      "placeholder": "​",
      "style": "IPY_MODEL_f067439c04b44402a405f3630850c808",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "eac348febe7b412cb506f51bc41c2f4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f067439c04b44402a405f3630850c808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f241641976f84d3f80d7ac62dfba73ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd987f8a375e42abb26fcea1e9cf9a8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
