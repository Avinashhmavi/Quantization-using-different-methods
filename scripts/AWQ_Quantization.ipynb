{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol3iypBOaPv9",
        "outputId": "af915060-76c0-475e-d7a4-1d4f5c8c6847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers  datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3FZriZAakQQ",
        "outputId": "4ed83c04-d65a-4517-c514-e7b91039a27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autoawq\n",
            "  Downloading autoawq-0.2.9.tar.gz (74 kB)\n",
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/74.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from autoawq) (2.6.0+cu124)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.2.0)\n",
            "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.51.3)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.21.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.13.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from autoawq) (1.6.0)\n",
            "Requirement already satisfied: datasets>=2.20 in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.6.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.23.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.26.5 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->autoawq) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->autoawq) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->autoawq) (5.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->autoawq) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->autoawq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\n",
            "Building wheels for collected packages: autoawq\n",
            "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autoawq: filename=autoawq-0.2.9-py3-none-any.whl size=115106 sha256=33ed8a8fa3cef6f39b3435e27850cdb3c8a173ab2a75c1d55c7f3c4c4d95526a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/31/e6/260073853a2419a05b7cd592d82db1e34abce58404854ef14d\n",
            "Successfully built autoawq\n",
            "Installing collected packages: autoawq\n",
            "Successfully installed autoawq-0.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip install autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GXLWuVungaCm",
        "outputId": "5677d09f-8620-4334-fc55-f56c9b90be8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AutoAWQ'...\n",
            "remote: Enumerating objects: 3627, done.\u001b[K\n",
            "remote: Counting objects: 100% (877/877), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 3627 (delta 752), reused 617 (delta 592), pack-reused 2750 (from 3)\u001b[K\n",
            "Receiving objects: 100% (3627/3627), 7.73 MiB | 18.76 MiB/s, done.\n",
            "Resolving deltas: 100% (2247/2247), done.\n",
            "/content/AutoAWQ\n",
            "Processing /content/AutoAWQ\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (2.6.0+cu124)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (3.2.0)\n",
            "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (4.51.3)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (0.21.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (4.13.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (1.6.0)\n",
            "Requirement already satisfied: datasets>=2.20 in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (3.6.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (0.23.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.26.5 in /usr/local/lib/python3.11/dist-packages (from autoawq==0.2.9) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq==0.2.9) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->autoawq==0.2.9) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->autoawq==0.2.9) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->autoawq==0.2.9) (5.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->autoawq==0.2.9) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->autoawq==0.2.9) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.9) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.9) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.9) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.9) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->autoawq==0.2.9) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq==0.2.9) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq==0.2.9) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq==0.2.9) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.20->autoawq==0.2.9) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq==0.2.9) (1.17.0)\n",
            "Building wheels for collected packages: autoawq\n",
            "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autoawq: filename=autoawq-0.2.9-py3-none-any.whl size=115106 sha256=60f97a418cb89245cbaa58f036a2eeb639eb69b34033cdc6ce9f322eef5e0841\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2zs0nnmd/wheels/c3/0f/9b/5555f912616cad6e0005ca1b9874c446401d17099c31d3a590\n",
            "Successfully built autoawq\n",
            "Installing collected packages: autoawq\n",
            "  Attempting uninstall: autoawq\n",
            "    Found existing installation: autoawq 0.2.9\n",
            "    Uninstalling autoawq-0.2.9:\n",
            "      Successfully uninstalled autoawq-0.2.9\n",
            "Successfully installed autoawq-0.2.9\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ca71bc15c284428496f86145638938e4",
              "pip_warning": {
                "packages": [
                  "awq"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!git clone https://github.com/casper-hansen/AutoAWQ\n",
        "%cd AutoAWQ\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-MCa1NlakTG",
        "outputId": "bcb552b3-435e-4012-8f39-8743d819bb3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_TOKEN=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\n"
          ]
        }
      ],
      "source": [
        "%env HF_TOKEN="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a0a4ba37d87d4e4eaef55c3e184760a6",
            "af116620fcbd49b19bfe00f4f28efd34",
            "72adf53af13144848d0d85d6569e10e8",
            "1bb5edaf419b4c399d233393ab72a399",
            "e502debb37f44da980107d41c68547cf",
            "93175ad6af104fb3b27dcb0384a86d68",
            "f87512cd1e2a40ac91674d61639bb177",
            "5921ac5f211946fabcf021a62f88960f",
            "a729d7505d084d29bd552fd23618e843",
            "b69663ef5e49451c97de18c610d915a1",
            "e590c912533f4fa59a3a3f41e98eee72",
            "a8d08149fb334ccf85129dca881d780b",
            "50a3dc9182794aa1b08aad59aa192d0a",
            "4480ff0e9aa346d6a659cacbad43ccdf",
            "46a5f704280048cabecf1b9113400bd1",
            "19b3a017051f49eb875bbb778262e2d2",
            "ba0fedd0372b4b10bde68ab92de6b7b6",
            "cbf077155c2d4ad6bfa3548c50a394f6",
            "77c68137fd1f49d891de6e9499fa3290",
            "5f6eeea6ce064c219831837249f36380",
            "415f44dac2044f138998eb8dcbc16cdf",
            "d6e008f4c20643a798eb49f927866776",
            "1fee1670f9b44783b3bce70f881fa12a",
            "c6093ef9ecd1432698bc8dac6c3e8a5d",
            "aeefabd49053460db961efbf04680b30",
            "7c3240d259c146eebcd4d7ba2b48728b",
            "ea5bd30013f7448a9006764cbdddc54e",
            "7c2cfba209f5491ba9fe51e8ec927cfd",
            "6542f82543954f31badfe7627dc867a5",
            "977beeaae1154a86aa2585c98309bff9",
            "aa902e6bcbee4a859b1467bcc3bb59f3",
            "48d4d9780fb14691a634481d6ba81281",
            "c91167200eb04b9883631df760aa71a7",
            "6c0a2e3540ba4640bf295c90bd65c70f",
            "dfafc605cf9c4d3fb89222cf8980839c",
            "feec7bab49674d258b40e90e7b831839",
            "ee37d3718a2a492ebbd77c29ab8e031b",
            "a14fd919012a43b08431a1d2f8a8d3e7",
            "81b73f69145344fb87527750ba592ecb",
            "5a19db5f3d1d446a8e9417a5ac07951d",
            "ea2a6163fc4949a4918f209c6a3b4c0a",
            "206efda6e0f74a2fa1c98bfb769efcef",
            "60ac33355c8d45c7bb2fd4107d09a2bd",
            "031eb37ce36c4613b9f46422d19300fe"
          ]
        },
        "id": "2wN3ZR6Rakba",
        "outputId": "3775e48f-1a6d-439a-e96a-aa9c75b1b8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading Tokenizer...\n",
            "\n",
            "Loading calibration examples...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0a4ba37d87d4e4eaef55c3e184760a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8d08149fb334ccf85129dca881d780b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Quantizing LLaMA Model with AWQ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fee1670f9b44783b3bce70f881fa12a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AWQ: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [09:50<00:00, 36.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving quantized model to 'llama-AWQ'...\n",
            "\n",
            "Saving tokenizer to 'llama-AWQ'...\n",
            "\n",
            "Verifying saved files...\n",
            "Found: model.safetensors\n",
            "Found: config.json\n",
            "Found: tokenizer.json\n",
            "\n",
            "Baseline GPU memory usage: 559.26 MB\n",
            "\n",
            "Loading Quantized LLaMA Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Replacing layers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:05<00:00,  3.00it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/awq/models/base.py:541: UserWarning: Skipping fusing modules because AWQ extension is not installed.No module named 'awq_ext'\n",
            "  warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model loaded. Memory usage: 1543.51 MB\n",
            "\n",
            "Loading Original LLaMA Model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c0a2e3540ba4640bf295c90bd65c70f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model loaded. Memory usage: 3900.64 MB\n",
            "\n",
            "Prompt 1: What is the capital of France, and what is its largest city?\n",
            "Original Response:\n",
            "Response: What is the capital of France, and what is its largest city? If these are questions that have been on your mind, then you have come to the right place. In this article, we will explore the answers to these questions and much more. We will also provide you with a list of other interesting facts about France and its capital, Paris. So, without further ado, let's get started!\n",
            "What city is France's capital?\n",
            "The answer to this question is Paris, which is also known as the \"City of Light\" and \"The City of Love.\"\n",
            "Inference Time: 8.7636 seconds\n",
            "Quantized Response:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: What is the capital of France, and what is its largest city? In what state was the Battle of Gettysburg fought? How many states and the District of Columbia are there?\n",
            "Inference Time: 18.6737 seconds\n",
            "\n",
            "Prompt 2: Write a short story about a robot exploring an abandoned city.\n",
            "Original Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city. The story should be no more than 1,000 words and should include the following elements: A robot is exploring a city that has been abandoned for many years. As the robot explores the city, it comes across a series of puzzles that must be solved in order to progress. Each puzzle has a unique solution that requires the use of a specific tool or piece of equipment.\n",
            "The robot must use these tools and equipment to solve the puzzles and progress to the next stage of the game. Along the way\n",
            "Inference Time: 8.2509 seconds\n",
            "Quantized Response:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Write a short story about a robot exploring an abandoned city. The story should be at least 1,000 words long and should include the following elements:\n",
            "1. A robot is exploring a city that has been abandoned for a long period of time.\n",
            "2. As the robot explores the city, it encounters various obstacles and challenges that it must overcome in order to complete its mission.\n",
            "3. Throughout the story, the reader will be introduced to a variety of characters, each with their own unique personalities and motivations.\n",
            "4. Each character will have a role to play\n",
            "Inference Time: 13.3547 seconds\n",
            "\n",
            "Prompt 3: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "Original Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours? (1 mile = 5,280 feet)\n",
            "A. 12,000 feet\n",
            "B. \u00a0 15,600 feet.\n",
            "Answer: B\n",
            "Explanation: The distance traveled is the product of the speed and the time.\n",
            "Inference Time: 3.9012 seconds\n",
            "Quantized Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours? (1 mile = 5,280 feet)\n",
            "A. 3.6 miles\n",
            "B.\u00a04.8 miles\u00a0\n",
            "C.\u00a0\u00a0 6.4 miles \u00a0\n",
            "D.\u00a0\u00a0\u00a0 8.0 miles\u00a0\u00a0\n",
            "E.\u00a0\u00a0\u00a0\u00a0 9.2 miles \u00a0\u00a0\n",
            "Answer: E\n",
            "Inference Time: 6.5859 seconds\n",
            "\n",
            "Prompt 4: Explain why the sky appears blue.\n",
            "Original Response:\n",
            "Response: Explain why the sky appears blue. The sky is blue because it is made up of a large number of different wavelengths of light. Blue light has a shorter wavelength than red light, so it can pass through the atmosphere more easily. This allows more blue light to reach the Earth's surface, which makes it appear blue.\n",
            "What is the reason for the blue sky?\n",
            "The blue color of the skies is due to the scattering of sunlight by atmospheric particles. These particles, such as dust and smoke, are suspended in the air and scatter the\n",
            "Inference Time: 8.2090 seconds\n",
            "Quantized Response:\n",
            "Response: Explain why the sky appears blue. The sky is blue because it is made up of a large number of different colors, each of which is reflected by the atmosphere in a different way. For example, blue light is refracted more than red light, so it bounces off the air molecules in the same way that a drop of water does. This causes the blue color to be reflected more strongly than the red color.\n",
            "Inference Time: 12.4298 seconds\n",
            "\n",
            "Prompt 5: What\u2019s your favorite book, and why?\n",
            "Original Response:\n",
            "Response: What\u2019s your favorite book, and why? It\u2019s a hard question to answer, because there are so many great books out there. But if I had to choose just one, it would be The Lord of the Rings by J.R.R. Tolkien. I read it when I was in high school and it changed my life. It introduced me to a whole new world of stories and characters that have stayed with me ever since. The books are set in a fantasy world called Middle-Earth, which is full of magic and mystery. There are\n",
            "Inference Time: 8.1865 seconds\n",
            "Quantized Response:\n",
            "Response: What\u2019s your favorite book, and why? Mine is \u201cThe Catcher in the Rye\u201d by J.D. Salinger. I read it when I was in high school and I still love it to this day. It\u2019s a classic for a reason.\n",
            "Inference Time: 12.8772 seconds\n",
            "\n",
            "Quantization and testing complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from transformers import AutoTokenizer\n",
        "from awq import AutoAWQForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model and output paths\n",
        "model_id = \"unsloth/Llama-3.2-1B\"  # Replace with your model ID\n",
        "quantized_model_dir = \"llama-AWQ\"\n",
        "\n",
        "# Example texts for quantization calibration\n",
        "def get_calibration_examples(num_examples=128):\n",
        "    \"\"\"Load example texts from C4 English dataset for quantization.\"\"\"\n",
        "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
        "    examples = []\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= num_examples:\n",
        "            break\n",
        "        text = example['text'][:512]  # Limit to 512 characters\n",
        "        examples.append(text)\n",
        "    return examples\n",
        "\n",
        "# Define prompts to test the model\n",
        "prompts = [\n",
        "    \"What is the capital of France, and what is its largest city?\",\n",
        "    \"Write a short story about a robot exploring an abandoned city.\",\n",
        "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
        "    \"Explain why the sky appears blue.\",\n",
        "    \"What\u2019s your favorite book, and why?\"\n",
        "]\n",
        "\n",
        "def verify_model_directory(model_dir):\n",
        "    \"\"\"Verify that the model directory contains required files.\"\"\"\n",
        "    required_files = ['model.safetensors', 'config.json', 'tokenizer.json']\n",
        "    return all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)\n",
        "\n",
        "def query_model(model, tokenizer, prompt, max_new_tokens=100, num_beams=5, temperature=0.5):\n",
        "    \"\"\"Query the model with a prompt and return the generated response with inference time.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    inference_time = time.perf_counter() - start_time\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip(), inference_time\n",
        "\n",
        "try:\n",
        "    # Check if quantized_model_dir exists and remove it\n",
        "    if os.path.exists(quantized_model_dir):\n",
        "        print(f\"\\nDirectory '{quantized_model_dir}' already exists. Deleting to create a fresh quantized model...\")\n",
        "        shutil.rmtree(quantized_model_dir)\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"\\nLoading Tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Get calibration examples\n",
        "    print(\"\\nLoading calibration examples...\")\n",
        "    examples = get_calibration_examples()\n",
        "\n",
        "    # Quantize the model using AWQ\n",
        "    print(\"\\nQuantizing LLaMA Model with AWQ...\")\n",
        "    quant_config = {\n",
        "        \"zero_point\": True,\n",
        "        \"q_group_size\": 128,\n",
        "        \"w_bit\": 4,\n",
        "        \"version\": \"GEMM\"\n",
        "    }\n",
        "    quantized_model = AutoAWQForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        use_auth_token=True\n",
        "    )\n",
        "    quantized_model.quantize(tokenizer, quant_config=quant_config, calib_data=examples)\n",
        "\n",
        "    # Save quantized model\n",
        "    print(f\"\\nSaving quantized model to '{quantized_model_dir}'...\")\n",
        "    quantized_model.save_quantized(quantized_model_dir, safetensors=True)\n",
        "\n",
        "    # Save tokenizer files\n",
        "    print(f\"\\nSaving tokenizer to '{quantized_model_dir}'...\")\n",
        "    tokenizer.save_pretrained(quantized_model_dir)\n",
        "\n",
        "    # Verify saved files\n",
        "    print(\"\\nVerifying saved files...\")\n",
        "    if not verify_model_directory(quantized_model_dir):\n",
        "        raise FileNotFoundError(f\"Failed to save required files in '{quantized_model_dir}'\")\n",
        "    saved_files = os.listdir(quantized_model_dir)\n",
        "    for f in ['model.safetensors', 'config.json', 'tokenizer.json']:\n",
        "        if f in saved_files:\n",
        "            print(f\"Found: {f}\")\n",
        "        else:\n",
        "            print(f\"Missing: {f}\")\n",
        "\n",
        "    # Measure baseline memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        baseline_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "        print(f\"\\nBaseline GPU memory usage: {baseline_memory:.2f} MB\")\n",
        "\n",
        "    # Load quantized model for testing\n",
        "    print(\"\\nLoading Quantized LLaMA Model...\")\n",
        "    model_awq = AutoAWQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        safetensors=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Quantized model loaded. Memory usage: {quantized_memory:.2f} MB\")\n",
        "\n",
        "    # Load original model for comparison\n",
        "    print(\"\\nLoading Original LLaMA Model...\")\n",
        "    model_original = AutoAWQForCausalLM.from_pretrained(model_id, use_auth_token=True, device_map='auto')\n",
        "    torch.cuda.synchronize()\n",
        "    original_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Original model loaded. Memory usage: {original_memory:.2f} MB\")\n",
        "\n",
        "    # Query both models\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\nPrompt {i}: {prompt}\")\n",
        "\n",
        "        # Query original model\n",
        "        print(\"Original Response:\")\n",
        "        response_original, time_original = query_model(model_original, tokenizer, prompt)\n",
        "        print(f\"Response: {response_original}\")\n",
        "        print(f\"Inference Time: {time_original:.4f} seconds\")\n",
        "\n",
        "        # Query quantized model\n",
        "        print(\"Quantized Response:\")\n",
        "        response_awq, time_awq = query_model(model_awq, tokenizer, prompt)\n",
        "        print(f\"Response: {response_awq}\")\n",
        "        print(f\"Inference Time: {time_awq:.4f} seconds\")\n",
        "\n",
        "    print(\"\\nQuantization and testing complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    traceback.print_exc()\n",
        "    print(\"Please ensure all dependencies are installed, the model ID is correct, and you have a valid Hugging Face token.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'quantized_model' in locals():\n",
        "        del quantized_model\n",
        "    if 'model_awq' in locals():\n",
        "        del model_awq\n",
        "    if 'model_original' in locals():\n",
        "        del model_original\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ee8c378231af41388b49bbe960412da1",
            "a3f74674d08d4e57a7d100b7f1b92d8d",
            "93af43a50a2d4bf7aa54bf44e0f53392",
            "a5100f0ef8a24ec4a18e309b31ebb8ce",
            "85d07620bed9427fb2e0c673c4269e94",
            "7e38f80147ae4385a4c439d1d72a3dda",
            "c98b4b14373f47beaa15855a90b26a90",
            "2f92aa72dd56405faf7240feaa9baea0",
            "e07c8043993d4a028a5e0cecdf6d93bf",
            "4a4af53f38b644e487fadcfa18e5df6e",
            "f7c27d65461843a782fc7e0b8df73950",
            "a0b05665344e44b88c43f334656eeba5",
            "7bb6ac4eac074b93a69f50c3013cfbf7",
            "f01a814793c442b49949a7fc1cefa473",
            "5f9e887ca9b448e7ac3eee907d43deb9",
            "3e7f14c290184106ba98e89780b74331",
            "33651d13d0504886a845176c7fc1cf5b",
            "9921040290e04756aab297d9b55b1055",
            "a8d35ca5c6dd42b7851ae46e8ac574c5",
            "f4fabfdaa3c84d09ae5fa3d108b46b3a",
            "e5f8f5248d104c11b2bcae1836d988ba",
            "136549dbca974c35ba787cd30494a2a7",
            "db85aa028c23441c81960571cbb1a1ee",
            "490938b335324bffaeae4419038fc9cf",
            "0f5fe308002c4868bc0dca0da53a45a0",
            "b836a4499ccc4af39b9c6a1bece482ff",
            "a769295cd1be4fcf9032f8f36a176039",
            "b821f0db2a244ad9bbd64cb58f915d77",
            "a254a1f2932a4a059a51f4f4f0c1513f",
            "61382389cdc84187afb3c511b1b8b82d",
            "928d6a4d9d4e44cd93e7ef385ce2ad8e",
            "2397ce6d9fd1422dbedde21029aa6b5b",
            "92693dd697b84c228646fe5b676eb89e",
            "d6d290434afc4700840db72dc494e1ea",
            "7208bfb0060f469b868b11b81913aa9c",
            "ede6c4259bbc4703adb9d90a2239d364",
            "5999208b98a24e61a527ba52d381f770",
            "7af3a51e62f74e47b0b315f6f09284f6",
            "67f1985d79b44a4488f7be7e6a079689",
            "9fa0d43f320b4f1bb0f90a25ae98080e",
            "5f863d0d03bf476d95d2949093d49a36",
            "f9c4a6a83d12431eb8111e2cdc8ed61c",
            "8923bc6942d842bab77423a2b4816a3b",
            "f06ed6e801de45e4b67cc65dfa1f40f6"
          ]
        },
        "id": "Sz6ng3Icc3lu",
        "outputId": "10be93c6-a5f3-4623-ff58-3ff70e733596"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Directory 'llama-AWQ' already exists. Deleting to create a fresh quantized model...\n",
            "\n",
            "Loading Tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading calibration examples...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee8c378231af41388b49bbe960412da1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0b05665344e44b88c43f334656eeba5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Quantizing LLaMA Model with AWQ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db85aa028c23441c81960571cbb1a1ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AWQ: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [09:53<00:00, 37.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving quantized model to 'llama-AWQ'...\n",
            "\n",
            "Saving tokenizer to 'llama-AWQ'...\n",
            "\n",
            "Verifying saved files...\n",
            "Found: model.safetensors\n",
            "Found: config.json\n",
            "Found: tokenizer.json\n",
            "\n",
            "Baseline GPU memory usage: 559.26 MB\n",
            "\n",
            "Loading Quantized LLaMA Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Replacing layers...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:05<00:00,  2.84it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/awq/models/base.py:541: UserWarning: Skipping fusing modules because AWQ extension is not installed.No module named 'awq_ext'\n",
            "  warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model loaded. Memory usage: 1542.51 MB\n",
            "\n",
            "Loading Original LLaMA Model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6d290434afc4700840db72dc494e1ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model loaded. Memory usage: 3900.52 MB\n",
            "\n",
            "Prompt 1: What is the capital of France, and what is its largest city?\n",
            "Original Response:\n",
            "Response: What is the capital of France, and what is its largest city? If these are questions that have been on your mind, then you have come to the right place. In this article, we will provide you with all the information you need to know about France and its capital, Paris.\n",
            "France is a country located in Western Europe. It has a population of over 67 million people and a total area of 643,801 square kilometers. The country is bordered by Belgium, Germany, Luxembourg, Switzerland, Italy, Spain and the Mediterranean Sea. France is also a member of the United Nations, the Council of Europe, NATO, G7 and G20. Its capital city is Paris, which is located on the Seine River. Other major cities in France include Lyon, Marseille, Toulouse, Lille, Bordeaux and Strasbourg. Paris is known as the \u201cCity of Light\u201d because it is home to many famous landmarks, including the Eiffel Tower, Arc de Triomphe, Louvre Museum and Notre Dame Cathedral.\n",
            "What Is The Capital Of\n",
            "Inference Time: 18.1827 seconds\n",
            "Quantized Response:\n",
            "Response: What is the capital of France, and what is its largest city??\n",
            "A. Aix-en-Provence, Lyon\n",
            "B. Paris, Marseille\n",
            "C. Nantes, Bordeaux\n",
            "D. Strasbourg, Toulouse\n",
            "Answer: B\n",
            "Explanation: Capitals and largest cities in each state and territory are listed in the Capitals & Largest Cities of the United States feature. The distance between each pair of states or territories is also given.  France is a country in Western Europe, bordered by the English Channel, the Mediterranean Sea, Switzerland, Italy, Austria, Germany, Luxembourg, Belgium, Netherlands, Poland, Czech Republic, Slovakia, Hungary, Romania, Serbia, Belarus, Ukraine, Armenia, Azerbaijan, Georgia, Turkey, Iran, Iraq, Syria, Lebanon, Jordan, Israel, Palestine, Egypt, Libya, Algeria, Tunisia, Morocco, Mauritania, Mali, Niger, Chad, Sudan, Ethiopia, Eritrea, Somalia, Djibouti, Kenya, Rwanda, Uganda, Tanzania, Zambia, Zimbabwe, Botswana, Namibia,\n",
            "Inference Time: 44.0275 seconds\n",
            "\n",
            "Prompt 2: Write a short story about a robot exploring an abandoned city.\n",
            "Original Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city. The story should have a beginning, a middle, and an end. You can use any setting you like, as long as it is a city that has been abandoned for a long time. Your story can be about anything that happens in the city, such as people, animals, robots, or anything else you can think of.\n",
            "Inference Time: 5.7728 seconds\n",
            "Quantized Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city. The story should be at least 1,000 words long, and it should have a beginning, a middle and an end. Your story can be set in any city in the world, as long as it is not a city that has already been written about in a previous story. You can use any setting you like, such as a futuristic city, an ancient city or a post-apocalyptic city.\n",
            "Your story must include the following elements:\n",
            "1. A robot that is exploring the city for the first time.\n",
            "2. An interaction between the robot and a human character.\n",
            "3. At least one scene where the human and robot interact with each other.\n",
            "4. Any other scenes that you want to include in your story.\n",
            "5. All of these scenes must be written in such a way that the reader can easily understand what is happening in each scene and how the story is progressing.\n",
            "How to Write a Short Story About a Robot Exploring an Abandoned City\n",
            "There are many ways to write a\n",
            "Inference Time: 40.8406 seconds\n",
            "\n",
            "Prompt 3: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "Original Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours? What is the average speed of the car?\n",
            "A. 20 miles per hour\n",
            "B. (60/1.25) miles = 48.0 miles\n",
            "Answer: B\n",
            "Explanation: The distance d is equal to the rate r times the time t.\n",
            "d = rt\n",
            "Inference Time: 5.2746 seconds\n",
            "Quantized Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours? What is the average speed of the car?\n",
            "A. 20 miles per hour\n",
            "B. (60/1.25) miles = 48 miles\n",
            "Answer: B\n",
            "Explanation: Speed is distance traveled in a given amount of time. The units used to measure speed are miles, meters, kilometers, or feet per second.\n",
            "Speed affects distance. For example, a race car can travel faster than a horse and buggy, but the buggy can go farther because of its larger wheels. If the same car races at a slower speed, it will travel a shorter distance.\n",
            "Whole number or fraction?The speed is written as a whole number if it is not part of a fraction. So 120 mph is three times as fast as 40 mph.\n",
            "Convert the speed?To learn how to convert a speed from one unit to another, check out this tutorial. Then, choose the equation that is right for the information you have and solve for what you're looking for.\n",
            "Inference Time: 41.9568 seconds\n",
            "\n",
            "Prompt 4: Explain why the sky appears blue.\n",
            "Original Response:\n",
            "Response: Explain why the sky appears blue. The sky is blue because it is made up of a large number of different wavelengths of light.\n",
            "A. true\n",
            "B. false\n",
            "Answer: A\n",
            "Inference Time: 7.8843 seconds\n",
            "Quantized Response:\n",
            "Response: Explain why the sky appears blue. The sky is blue because it is made up of a large number of different colors, each of which is reflected by the atmosphere in a different way. For example, blue light is refracted more than red light, so it appears to come from a greater distance than it actually does.\n",
            "What is the reason for the blue sky?\n",
            "The blue color is due to the presence of water vapor in the air, which causes the light to refract (bend) as it passes through the water droplets. This phenomenon is known as the Rayleigh scattering of light.\n",
            "Why is sky blue and why is it called blue?\n",
            "Blue sky refers to a sky that is clear and free of clouds. It is a term that has been used since the 16th century and is derived from the Latin word \u201ccaelum\u201d, which means \u201csky\u201d. The term \u201cblue sky\u201d was first used in English in 1611, when it was used to describe a clear sky.\n",
            "How do you explain the\n",
            "Inference Time: 40.1865 seconds\n",
            "\n",
            "Prompt 5: What\u2019s your favorite book, and why?\n",
            "Original Response:\n",
            "Response: What\u2019s your favorite book, and why? It\u2019s hard to pick just one, but if I had to choose, it would have to be \u201cThe Lord of the Rings\u201d by J.R.R. Tolkien. I read it when I was in high school and it was one of my favorite books of all time. The characters are so well-developed and the story is so captivating that I couldn\u2019t put it down. It was the first book I ever read that made me fall in love with the written word.\n",
            "What do you like to do in your free time? I love to spend time with my family and friends. We have a lot of fun playing games and watching movies together. In the summer, I enjoy going to the beach and swimming in the ocean. During the winter, we usually go skiing or snowboarding.\n",
            "Do you have any pets? If so, what are their names and what do they look like?\n",
            "I have two dogs, a German Shepherd and a Golden Retriever. They are both very friendly and love\n",
            "Inference Time: 17.8862 seconds\n",
            "Quantized Response:\n",
            "Response: What\u2019s your favorite book, and why? Mine is The Catcher in the Rye by J.D. Salinger. I read it when I was in high school and it was one of the most influential books I\u2019ve ever read. It changed the way I look at the world and the people around me.\n",
            "What do you like to do in your free time? I love to read, watch movies, listen to music and play video games.\n",
            "If you could have dinner with anyone (living or dead), who would it be and what would you talk about? My favorite author is J.R.R. Tolkien and I would ask him about his writing process and how he came up with the names of his characters.\n",
            "Inference Time: 34.5810 seconds\n",
            "\n",
            "Quantization and testing complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from transformers import AutoTokenizer\n",
        "from awq import AutoAWQForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model and output paths\n",
        "model_id = \"unsloth/Llama-3.2-1B\"\n",
        "quantized_model_dir = \"llama-AWQ\"\n",
        "\n",
        "# Example texts for quantization calibration\n",
        "def get_calibration_examples(num_examples=256):  # Increased to 256\n",
        "    \"\"\"Load example texts from C4 English dataset for quantization.\"\"\"\n",
        "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
        "    examples = []\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= num_examples:\n",
        "            break\n",
        "        text = example['text'][:512]\n",
        "        examples.append(text)\n",
        "    return examples\n",
        "\n",
        "# Define prompts to test the model\n",
        "prompts = [\n",
        "    \"What is the capital of France, and what is its largest city?\",\n",
        "    \"Write a short story about a robot exploring an abandoned city.\",\n",
        "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
        "    \"Explain why the sky appears blue.\",\n",
        "    \"What\u2019s your favorite book, and why?\"\n",
        "]\n",
        "\n",
        "def verify_model_directory(model_dir):\n",
        "    \"\"\"Verify that the model directory contains required files.\"\"\"\n",
        "    required_files = ['model.safetensors', 'config.json', 'tokenizer.json']\n",
        "    return all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)\n",
        "\n",
        "def query_model(model, tokenizer, prompt, max_new_tokens=200, num_beams=10):  # Increased for better quality\n",
        "    \"\"\"Query the model with a prompt and return the generated response with inference time.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=num_beams,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    inference_time = time.perf_counter() - start_time\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip(), inference_time\n",
        "\n",
        "try:\n",
        "    # Check if quantized_model_dir exists and remove it\n",
        "    if os.path.exists(quantized_model_dir):\n",
        "        print(f\"\\nDirectory '{quantized_model_dir}' already exists. Deleting to create a fresh quantized model...\")\n",
        "        shutil.rmtree(quantized_model_dir)\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"\\nLoading Tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Get calibration examples\n",
        "    print(\"\\nLoading calibration examples...\")\n",
        "    examples = get_calibration_examples()\n",
        "\n",
        "    # Quantize the model using AWQ\n",
        "    print(\"\\nQuantizing LLaMA Model with AWQ...\")\n",
        "    quant_config = {\n",
        "        \"zero_point\": True,\n",
        "        \"q_group_size\": 128,\n",
        "        \"w_bit\": 4,\n",
        "        \"version\": \"GEMM\"\n",
        "    }\n",
        "    quantized_model = AutoAWQForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        token=True\n",
        "    )\n",
        "    quantized_model.quantize(tokenizer, quant_config=quant_config, calib_data=examples)\n",
        "\n",
        "    # Save quantized model\n",
        "    print(f\"\\nSaving quantized model to '{quantized_model_dir}'...\")\n",
        "    quantized_model.save_quantized(quantized_model_dir, safetensors=True)\n",
        "\n",
        "    # Save tokenizer files\n",
        "    print(f\"\\nSaving tokenizer to '{quantized_model_dir}'...\")\n",
        "    tokenizer.save_pretrained(quantized_model_dir)\n",
        "\n",
        "    # Verify saved files\n",
        "    print(\"\\nVerifying saved files...\")\n",
        "    if not verify_model_directory(quantized_model_dir):\n",
        "        raise FileNotFoundError(f\"Failed to save required files in '{quantized_model_dir}'\")\n",
        "    saved_files = os.listdir(quantized_model_dir)\n",
        "    for f in ['model.safetensors', 'config.json', 'tokenizer.json']:\n",
        "        if f in saved_files:\n",
        "            print(f\"Found: {f}\")\n",
        "        else:\n",
        "            print(f\"Missing: {f}\")\n",
        "\n",
        "    # Measure baseline memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        baseline_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "        print(f\"\\nBaseline GPU memory usage: {baseline_memory:.2f} MB\")\n",
        "\n",
        "    # Load quantized model for testing\n",
        "    print(\"\\nLoading Quantized LLaMA Model...\")\n",
        "    model_awq = AutoAWQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        safetensors=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Quantized model loaded. Memory usage: {quantized_memory:.2f} MB\")\n",
        "\n",
        "    # Load original model for comparison\n",
        "    print(\"\\nLoading Original LLaMA Model...\")\n",
        "    model_original = AutoAWQForCausalLM.from_pretrained(model_id, token=True, device_map='auto')\n",
        "    torch.cuda.synchronize()\n",
        "    original_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Original model loaded. Memory usage: {original_memory:.2f} MB\")\n",
        "\n",
        "    # Query both models\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\nPrompt {i}: {prompt}\")\n",
        "\n",
        "        # Query original model\n",
        "        print(\"Original Response:\")\n",
        "        response_original, time_original = query_model(model_original, tokenizer, prompt)\n",
        "        print(f\"Response: {response_original}\")\n",
        "        print(f\"Inference Time: {time_original:.4f} seconds\")\n",
        "\n",
        "        # Query quantized model\n",
        "        print(\"Quantized Response:\")\n",
        "        response_awq, time_awq = query_model(model_awq, tokenizer, prompt)\n",
        "        print(f\"Response: {response_awq}\")\n",
        "        print(f\"Inference Time: {time_awq:.4f} seconds\")\n",
        "\n",
        "    print(\"\\nQuantization and testing complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    import traceback\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    traceback.print_exc()\n",
        "    print(\"Please ensure all dependencies are installed, the model ID is correct, and you have a valid Hugging Face token.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'quantized_model' in locals():\n",
        "        del quantized_model\n",
        "    if 'model_awq' in locals():\n",
        "        del model_awq\n",
        "    if 'model_original' in locals():\n",
        "        del model_original\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVBf4mYLgpDy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}