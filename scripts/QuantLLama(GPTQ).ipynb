{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpmYwwPqyn4K",
        "outputId": "677ab638-65c3-48a7-94de-fef9692aa760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting auto_gptq\n",
            "  Downloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (1.6.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (0.2.0)\n",
            "Collecting rouge (from auto_gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto_gptq)\n",
            "  Downloading gekko-1.3.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto_gptq) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto_gptq) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gekko-1.3.0-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, rouge, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gekko, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, auto_gptq\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed auto_gptq-0.7.1 datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 gekko-1.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-1.0.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch transformers auto_gptq datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL-t8Rh2_u4y",
        "outputId": "c9b290b5-b8f5-408c-b6f1-b78b9dbea01d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r3q1iyHAa2h",
        "outputId": "2b6abace-96ff-41dc-97d2-e5c6a8892986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_TOKEN=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\n"
          ]
        }
      ],
      "source": [
        "%env HF_TOKEN=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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5ad45b002b02450092b9cbf315abcb09",
            "b62103e2c2344258af57d9eda63b0c19",
            "3c4152ef26de47a1aa586b89696b716b",
            "d42726178b004f3fb8282a566727850d",
            "fb4410190ec5456b8cce92c900a97210",
            "4ebc9d23ad5740438a269204b1e76c8b",
            "791e92f6ba114e1fbd8396a401d7febb",
            "3482df94134b467380ce43947475166b",
            "c95d526a0b344d9197f769d97f0bb854",
            "004610a6dc9e4f53a76a36b5419308a8",
            "688ea9a6ed7b498580ab725ed0bb49e7",
            "5e5a6f30d12b471091443e9766f1e787",
            "90d60627893f4eb8a37ae6e2615ee216",
            "c7689618ecbb4985b3ec4b6ffaf0da13",
            "5269c2b3443c4d0a9fb2363c31ee07ca",
            "350fd71ed2854b1ab37dd0c4bc4780be",
            "3344d90fd7c54b0cb8dd9f4d2be9f2ea",
            "70aaa885bd4642a5a8a9a31a072f5a1f",
            "2311215f85d5445fb3fa62292d5d4c90",
            "26e508b24c8f496685a9e80a3e399f9a",
            "b5b66cadb5244ed092cc42f155d77b1b",
            "16056c796795428f9064199a88f9b364",
            "747897ca3caa4829b9383e1f15254b77",
            "8bc2f13812a2407f8df680f13946e750",
            "42884e156171401985fb84605a8b6d71",
            "7452573d1ee64b858266e2929d89a4aa",
            "3a154b269bac44a6bd20bbe9a01e8da0",
            "1370361122e64d9381f5af984ed9b036",
            "3e2957ca7b78472eb825821b09d20cef",
            "ef273217ab494bd698747113a2b0cbb2",
            "75b631a50cdc425999f6e52309d7d615",
            "d7a9c6d675d0413d808941a2ce77a075",
            "f11bd4296ac1463eb66c1d40aa9dc052",
            "6978a3a4f27d42caa1758ee621f3d613",
            "5c50d93a2c1c40209c5e6e072e454f3c",
            "561baffce8f9480782bd2ea04940f96e",
            "d88f8b75c1ff42e6b7ec549114531c57",
            "9c8981f4183645daaa4a9e5da7a7f9f0",
            "96848b616490451bbf4772b129807562",
            "26e30c19dff64ae0901caed6e39a4731",
            "26208b07154940ea94a0b14188292685",
            "a76b8ad4456942259c01c3d0ef75769f",
            "8c05cffdde534df480fb72f5f340ac82",
            "ebe1386c0ae846d3be53c6319bc9cc58",
            "8024712620c24c54b0d0c12e496ff3ae",
            "38c902d06f1240a9a653e1194ba7ba45",
            "bd231bdce19e47b886eb81d27a021fbe",
            "dfef6cbb046d48b082da7cb5d1648547",
            "1e76662a79f94d4bb286e19897d275ff",
            "f8a7977435104eaca1c6406d82d6dad9",
            "b1b731f6229241febc9ffb0bd2eae61c",
            "ba7856915460442f93e98b8dc37bb7e7",
            "c198943579a6444397be7e11ddcf553f",
            "e102f426e62e47d09e6cfb1266fea5da",
            "b9744e4b732d49dd99a9afba16d45718",
            "b2f98e3cd9e646b084afc68a4385b030",
            "a42f4ee4eb474a5792f3ce0ff25fbcd1",
            "b024d604d23249119d316ce17ce22648",
            "a1215f3ce1694c15b80da7832cd463b1",
            "737f14b3f3624289a7c2dd539e284e72",
            "03131104c53e4a90b32600ae88817ff9",
            "2f48434409654c449285857c19c84a69",
            "3db612d873bd4a4a955b646da1995827",
            "c2997747cdbd4f04bae1df2c3f8f41a7",
            "23d82306a25b4af88cedf2e77e86466d",
            "16db79ba01fd442baeafa1dec9e2e6b2",
            "aae6effd99f04d6fb191527beeb67248",
            "fd601758bd7144c491b7e2938cad9e90",
            "2b83d70cf3ce4e989ab260c435911891",
            "eeb1e714e3ac4f28a26d941829f40556",
            "3056aef535ab454d867a7e985c5f21d0",
            "408be5edd076441390bd126a213b8dd1",
            "77551d6192df429f81c638b78f0e8da0",
            "94129b5af2aa45d8b93bfcbe49091fed",
            "c19c606afa964571b0c89f63f0b950eb",
            "f0962d93da8849f7a437f272c10b73dc",
            "d6bfabf295bc4a7893b95dbf4d31644b",
            "2df9e125acb14cf29a5d914a739b19a8",
            "3553bdf08a9f45b1acfa0264a407a023",
            "3252578489214d409037e78303e171fa",
            "2d1682f8066d49589f4fb4f8bc97adc0",
            "d7254130d52046c8b1cb387f9707aa87",
            "51dc8bfa118d4404939892f0625d486f",
            "1f3643d8c938471fa1eeb8cc0c36dc17",
            "f9616607afb9499387be0a9fcb02cbcb",
            "5d009cc7429f467695d01c390233b773",
            "5ab3e02cc17244ffae16037cdbb19f51",
            "9036eb5f021b4aabac8e0be9cc1d561d",
            "7df4859013d64e47bce786682d6e825c",
            "e413de0c459a44eeab333081fe6f94c2",
            "e029e4cc21584c0e8b5b37cf49dbacce",
            "83fb7ec4f7944aa08511991ebe4dd0c2",
            "f3ce8b29613d4350bb04ad225d46792e",
            "05e26659358741de9cbee180f89969dc",
            "6b1c8c0b46404a53bf83016a08eef08b",
            "4cfa714a1a874edf88222adf76d2e95e",
            "db6ac3299183437a96f697803872c7b0",
            "0b98e3c701f64a95a53f5ab12cc576d8",
            "c7cdd78dc3c74ecbace1b2284d3daf97"
          ]
        },
        "id": "L3SOuGchArO9",
        "outputId": "5cccd42e-aa9a-49ca-e0ed-4418ff929f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading LLaMA Model and Tokenizer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ad45b002b02450092b9cbf315abcb09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e5a6f30d12b471091443e9766f1e787",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "747897ca3caa4829b9383e1f15254b77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6978a3a4f27d42caa1758ee621f3d613",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8024712620c24c54b0d0c12e496ff3ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2f98e3cd9e646b084afc68a4385b030",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading calibration examples...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aae6effd99f04d6fb191527beeb67248",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/41.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2df9e125acb14cf29a5d914a739b19a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7df4859013d64e47bce786682d6e825c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Quantizing LLaMA Model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "INFO - Start quantizing layer 1/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 1/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 1/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 1/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 1/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 1/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 1/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 1/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 1/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 1/16...\n",
            "INFO - Start quantizing layer 2/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 2/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 2/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 2/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 2/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 2/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 2/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 2/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 2/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 2/16...\n",
            "INFO - Start quantizing layer 3/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 3/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 3/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 3/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 3/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 3/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 3/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 3/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 3/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 3/16...\n",
            "INFO - Start quantizing layer 4/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 4/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 4/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 4/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 4/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 4/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 4/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 4/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 4/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 4/16...\n",
            "INFO - Start quantizing layer 5/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 5/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 5/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 5/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 5/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 5/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 5/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 5/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 5/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 5/16...\n",
            "INFO - Start quantizing layer 6/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 6/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 6/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 6/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 6/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 6/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 6/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 6/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 6/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 6/16...\n",
            "INFO - Start quantizing layer 7/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 7/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 7/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 7/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 7/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 7/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 7/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 7/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 7/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 7/16...\n",
            "INFO - Start quantizing layer 8/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 8/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 8/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 8/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 8/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 8/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 8/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 8/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 8/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 8/16...\n",
            "INFO - Start quantizing layer 9/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 9/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 9/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 9/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 9/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 9/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 9/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 9/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 9/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 9/16...\n",
            "INFO - Start quantizing layer 10/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 10/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 10/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 10/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 10/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 10/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 10/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 10/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 10/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 10/16...\n",
            "INFO - Start quantizing layer 11/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 11/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 11/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 11/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 11/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 11/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 11/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 11/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 11/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 11/16...\n",
            "INFO - Start quantizing layer 12/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 12/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 12/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 12/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 12/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 12/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 12/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 12/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 12/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 12/16...\n",
            "INFO - Start quantizing layer 13/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 13/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 13/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 13/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 13/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 13/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 13/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 13/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 13/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 13/16...\n",
            "INFO - Start quantizing layer 14/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 14/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 14/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 14/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 14/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 14/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 14/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 14/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 14/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 14/16...\n",
            "INFO - Start quantizing layer 15/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 15/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 15/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 15/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 15/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 15/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 15/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 15/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 15/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 15/16...\n",
            "INFO - Start quantizing layer 16/16\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 16/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 16/16...\n",
            "INFO - Quantizing self_attn.v_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 16/16...\n",
            "INFO - Quantizing self_attn.q_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 16/16...\n",
            "INFO - Quantizing self_attn.o_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 16/16...\n",
            "INFO - Quantizing mlp.up_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 16/16...\n",
            "INFO - Quantizing mlp.gate_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 16/16...\n",
            "INFO - Quantizing mlp.down_proj in layer 16/16...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 16/16...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving quantized model to 'llama-GPTQ'...\n",
            "\n",
            "Saving tokenizer to 'llama-GPTQ'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Renamed 'llama-GPTQ/gptq_model-4bit-128g.safetensors' to 'llama-GPTQ/model.safetensors' for compatibility.\n",
            "\n",
            "Verifying saved files...\n",
            "Found: model.safetensors\n",
            "Found: config.json\n",
            "Found: tokenizer.json\n",
            "\n",
            "Baseline GPU memory usage: 4776.94 MB\n",
            "\n",
            "Loading Quantized LLaMA Model...\n",
            "Quantized model loaded. Memory usage: 5708.85 MB\n",
            "\n",
            "Loading Original LLaMA Model...\n",
            "Original model loaded. Memory usage: 10423.11 MB\n",
            "\n",
            "Prompt 1: What is the capital of France, and what is its largest city?\n",
            "Original Response:\n",
            "Response: What is the capital of France, and what is its largest city? If these are questions that have been on your mind, then you have come to the right place. In this article, we will answer these questions and provide you with all the information you need to know about the French capital and its biggest city. We will also discuss some interesting facts about both of these cities. So, if you are interested in learning more about them, keep reading!\n",
            "What Is The Capital Of France?\n",
            "The capital is Paris, which is located in the Ile-de-France region\n",
            "Inference Time: 4.5696 seconds\n",
            "Quantized Response:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: What is the capital of France, and what is its largest city? Find the answers to these questions and more with a little help from your friends in France.\n",
            "A. Paris\n",
            "B. Bordeaux\n",
            "C. Lyon\n",
            "D. Marseille\n",
            "Answer: A\n",
            "Explanation: People often travel to different places and learn about their cultures. Geography Project coaches students as they choose a place to research. Then, using the map and the list of cities, place markers on the world map to show where each place is located.\n",
            "Inference Time: 10.9780 seconds\n",
            "\n",
            "Prompt 2: Write a short story about a robot exploring an abandoned city.\n",
            "Original Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city. The story should have a beginning, middle, and end. You can use any setting you like, but the story must take place in a city that has been abandoned for at least 10 years. Your story can be as long or as short as you want. It is up to you to decide how long it should be.\n",
            "Inference Time: 2.5769 seconds\n",
            "Quantized Response:\n",
            "Response: Write a short story about a robot exploring an abandoned city. The story should have a beginning, middle, and end. You can use any style you want, but make sure it\u2019s clear what the story is about and how it ends.\n",
            "The robot is exploring a city that has been abandoned for a long time. It\u2019s a place where people used to live but now there are only a few people left. As the robot explores the city, it starts to wonder what happened to all of the people who lived there before it was abandoned.\n",
            "At first, the\n",
            "Inference Time: 11.8616 seconds\n",
            "\n",
            "Prompt 3: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\n",
            "Original Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours? The answer is 150 miles. The distance traveled is the product of the speed and the time. In this case, the distance is equal to the number of miles multiplied by the amount of time it takes to travel that distance.\n",
            "How do you calculate distance?\n",
            "The distance between two points can be calculated using the Pythagorean theorem. This theorem states that the square of a hypotenuse (the side opposite the right angle) equals the sum of squares of two other sides. For example, if\n",
            "Inference Time: 3.9241 seconds\n",
            "Quantized Response:\n",
            "Response: If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours? If the speed is constant, the distance traveled will be the same. However, if the car is traveling at a constant speed, it will take twice as long to travel 100 miles as it would have if it were traveling 50 miles per hour.\n",
            "Inference Time: 11.9497 seconds\n",
            "\n",
            "Prompt 4: Explain why the sky appears blue.\n",
            "Original Response:\n",
            "Response: Explain why the sky appears blue. What is the reason for this color?\n",
            "The sky is blue because it is made up of different colors of light. Blue light has a longer wavelength than red light, so it can pass through the atmosphere more easily. This means that more blue light reaches the Earth's surface, and it appears to be blue.\n",
            "Why do we see blue sky?\n",
            "Blue is a color that is created by light with a wavelength of 450 to 490 nanometers (nm). This wavelength is longer than the wavelengths of\n",
            "Inference Time: 4.0849 seconds\n",
            "Quantized Response:\n",
            "Response: Explain why the sky appears blue. The sky is blue because it is made up of a large number of tiny droplets of water vapor that are suspended in a thin layer of air. When you look up at the night sky, you are actually looking at a very thin sheet of glass through which the light from the stars is reflected.\n",
            "Why do we see blue sky at night?\n",
            "The blue color is due to the scattering of light by the tiny particles of dust and water in the atmosphere. At night, when there is little or no\n",
            "Inference Time: 11.9820 seconds\n",
            "\n",
            "Prompt 5: What\u2019s your favorite book, and why?\n",
            "Original Response:\n",
            "Response: What\u2019s your favorite book, and why? It\u2019s a hard question to answer, because there are so many great books out there. But if I had to pick one, it would have to be The Lord of the Rings by J.R.R. Tolkien. I read it when I was in high school and it changed my life. It introduced me to a whole new world of fantasy and adventure. The characters were so well-developed and the story was so compelling that I couldn\u2019t put it down. And the fact that it was set in\n",
            "Inference Time: 3.8953 seconds\n",
            "Quantized Response:\n",
            "Response: What\u2019s your favorite book, and why? This is a tough question to answer, because I have so many books that I love. It\u2019s hard to narrow it down to just one, but I\u2019ll give it my best shot.\n",
            "The first book that comes to mind is The Lord of the Rings by J.R.R. Tolkien. I read this book when I was about 10 years old and it had a huge impact on me. The story is set in a fantasy world and follows the journey of a young hobbit named Frodo B\n",
            "Inference Time: 12.0766 seconds\n",
            "\n",
            "Quantization and testing complete.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define model and output paths\n",
        "model_id = \"unsloth/Llama-3.2-1B\"  # Replace with your LLaMA model ID\n",
        "quantized_model_dir = \"llama-GPTQ\"\n",
        "\n",
        "# Example texts for quantization calibration\n",
        "def get_calibration_examples(num_examples=128):\n",
        "    \"\"\"Load example texts from C4 English dataset for quantization.\"\"\"\n",
        "    dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
        "    examples = []\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= num_examples:\n",
        "            break\n",
        "        text = example['text'][:512]  # Limit to 512 characters\n",
        "        examples.append(text)\n",
        "    return examples\n",
        "\n",
        "# Define prompts to test the model\n",
        "prompts = [\n",
        "    \"What is the capital of France, and what is its largest city?\",\n",
        "    \"Write a short story about a robot exploring an abandoned city.\",\n",
        "    \"If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?\",\n",
        "    \"Explain why the sky appears blue.\",\n",
        "    \"What\u2019s your favorite book, and why?\"\n",
        "]\n",
        "\n",
        "def verify_model_directory(model_dir):\n",
        "    \"\"\"Verify that the model directory contains required files.\"\"\"\n",
        "    required_files = ['model.safetensors', 'config.json', 'tokenizer.json']\n",
        "    return all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)\n",
        "\n",
        "def query_model(model, tokenizer, prompt, max_new_tokens=100, num_beams=5, temperature=0.5):\n",
        "    \"\"\"Query the model with a prompt and return the generated response with inference time.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    start_time = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "    inference_time = time.perf_counter() - start_time\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip(), inference_time\n",
        "\n",
        "try:\n",
        "    # Check if quantized_model_dir exists and remove it\n",
        "    if os.path.exists(quantized_model_dir):\n",
        "        print(f\"\\nDirectory '{quantized_model_dir}' already exists. Deleting to create a fresh quantized model...\")\n",
        "        shutil.rmtree(quantized_model_dir)\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    print(\"\\nLoading LLaMA Model and Tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)  # Set your HF token\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Avoid pad token warnings\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=True).to(device)\n",
        "\n",
        "    # Prepare quantization configuration\n",
        "    quantize_config = BaseQuantizeConfig(\n",
        "        bits=4,          # 4-bit quantization\n",
        "        group_size=128,  # Group size for quantization\n",
        "        damp_percent=0.01,  # Damping factor\n",
        "        desc_act=False   # Disable act-order for stability\n",
        "    )\n",
        "\n",
        "    # Get calibration examples\n",
        "    print(\"\\nLoading calibration examples...\")\n",
        "    examples = get_calibration_examples()\n",
        "\n",
        "    # Tokenize examples\n",
        "    tokenized_examples = [tokenizer(ex, return_tensors='pt', padding=True, truncation=True).to(device) for ex in examples]\n",
        "\n",
        "    # Quantize the model\n",
        "    print(\"\\nQuantizing LLaMA Model...\")\n",
        "    quantized_model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config, device_map='cuda', use_auth_token=True)\n",
        "    quantized_model.quantize(tokenized_examples, use_triton=False)  # Triton may require CUDA kernels\n",
        "\n",
        "    # Save quantized model\n",
        "    print(f\"\\nSaving quantized model to '{quantized_model_dir}'...\")\n",
        "    quantized_model.save_quantized(quantized_model_dir, use_safetensors=True, safetensors_metadata={'format': 'pt'})\n",
        "\n",
        "    # Save tokenizer files\n",
        "    print(f\"\\nSaving tokenizer to '{quantized_model_dir}'...\")\n",
        "    tokenizer.save_pretrained(quantized_model_dir)\n",
        "\n",
        "    # Rename safetensors file to match expected name\n",
        "    old_safetensors = os.path.join(quantized_model_dir, 'gptq_model-4bit-128g.safetensors')\n",
        "    new_safetensors = os.path.join(quantized_model_dir, 'model.safetensors')\n",
        "    if os.path.exists(old_safetensors):\n",
        "        os.rename(old_safetensors, new_safetensors)\n",
        "        print(f\"Renamed '{old_safetensors}' to '{new_safetensors}' for compatibility.\")\n",
        "\n",
        "    # Verify saved files\n",
        "    print(\"\\nVerifying saved files...\")\n",
        "    if not verify_model_directory(quantized_model_dir):\n",
        "        raise FileNotFoundError(f\"Failed to save required files in '{quantized_model_dir}'\")\n",
        "    saved_files = os.listdir(quantized_model_dir)\n",
        "    for f in ['model.safetensors', 'config.json', 'tokenizer.json']:\n",
        "        if f in saved_files:\n",
        "            print(f\"Found: {f}\")\n",
        "        else:\n",
        "            print(f\"Missing: {f}\")\n",
        "\n",
        "    # Measure baseline memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        baseline_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "        print(f\"\\nBaseline GPU memory usage: {baseline_memory:.2f} MB\")\n",
        "\n",
        "    # Load quantized model for testing\n",
        "    print(\"\\nLoading Quantized LLaMA Model...\")\n",
        "    model_gptq = AutoGPTQForCausalLM.from_quantized(\n",
        "        quantized_model_dir,\n",
        "        use_safetensors=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Quantized model loaded. Memory usage: {quantized_memory:.2f} MB\")\n",
        "\n",
        "    # Load original model for comparison\n",
        "    print(\"\\nLoading Original LLaMA Model...\")\n",
        "    model_original = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=True).to(device)\n",
        "    torch.cuda.synchronize()\n",
        "    original_memory = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "    print(f\"Original model loaded. Memory usage: {original_memory:.2f} MB\")\n",
        "\n",
        "    # Query both models\n",
        "    for i, prompt in enumerate(prompts, 1):\n",
        "        print(f\"\\nPrompt {i}: {prompt}\")\n",
        "\n",
        "        # Query original model\n",
        "        print(\"Original Response:\")\n",
        "        response_original, time_original = query_model(model_original, tokenizer, prompt)\n",
        "        print(f\"Response: {response_original}\")\n",
        "        print(f\"Inference Time: {time_original:.4f} seconds\")\n",
        "\n",
        "        # Query quantized model\n",
        "        print(\"Quantized Response:\")\n",
        "        response_gptq, time_gptq = query_model(model_gptq, tokenizer, prompt)\n",
        "        print(f\"Response: {response_gptq}\")\n",
        "        print(f\"Inference Time: {time_gptq:.4f} seconds\")\n",
        "\n",
        "    print(\"\\nQuantization and testing complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"Please ensure all dependencies are installed, the model ID is correct, and you have a valid Hugging Face token.\")\n",
        "\n",
        "finally:\n",
        "    # Clean up\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "    if 'quantized_model' in locals():\n",
        "        del quantized_model\n",
        "    if 'model_gptq' in locals():\n",
        "        del model_gptq\n",
        "    if 'model_original' in locals():\n",
        "        del model_original\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}