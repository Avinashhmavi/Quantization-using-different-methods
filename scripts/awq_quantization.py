# -*- coding: utf-8 -*-
"""AWQ Quantization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yX7jOAN0p_0Z97SYlgHi1ju5CZOsbf0V
"""

!pip install torch transformers  datasets

!pip install autoawq

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/casper-hansen/AutoAWQ
# %cd AutoAWQ
!pip install .

# Commented out IPython magic to ensure Python compatibility.
# %env HF_TOKEN=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

import torch
import os
import shutil
import time
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM
from datasets import load_dataset

# Set random seed for reproducibility
torch.manual_seed(0)

# Check for GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Define model and output paths
model_id = "unsloth/Llama-3.2-1B"  # Replace with your model ID
quantized_model_dir = "llama-AWQ"

# Example texts for quantization calibration
def get_calibration_examples(num_examples=128):
    """Load example texts from C4 English dataset for quantization."""
    dataset = load_dataset("allenai/c4", "en", split="train", streaming=True)
    examples = []
    for i, example in enumerate(dataset):
        if i >= num_examples:
            break
        text = example['text'][:512]  # Limit to 512 characters
        examples.append(text)
    return examples

# Define prompts to test the model
prompts = [
    "What is the capital of France, and what is its largest city?",
    "Write a short story about a robot exploring an abandoned city.",
    "If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?",
    "Explain why the sky appears blue.",
    "What’s your favorite book, and why?"
]

def verify_model_directory(model_dir):
    """Verify that the model directory contains required files."""
    required_files = ['model.safetensors', 'config.json', 'tokenizer.json']
    return all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)

def query_model(model, tokenizer, prompt, max_new_tokens=100, num_beams=5, temperature=0.5):
    """Query the model with a prompt and return the generated response with inference time."""
    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']

    start_time = time.perf_counter()
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,
            temperature=temperature,
            pad_token_id=tokenizer.pad_token_id,
            early_stopping=True,
            no_repeat_ngram_size=2
        )
    inference_time = time.perf_counter() - start_time

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.strip(), inference_time

try:
    # Check if quantized_model_dir exists and remove it
    if os.path.exists(quantized_model_dir):
        print(f"\nDirectory '{quantized_model_dir}' already exists. Deleting to create a fresh quantized model...")
        shutil.rmtree(quantized_model_dir)

    # Load tokenizer
    print("\nLoading Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
    tokenizer.pad_token = tokenizer.eos_token

    # Get calibration examples
    print("\nLoading calibration examples...")
    examples = get_calibration_examples()

    # Quantize the model using AWQ
    print("\nQuantizing LLaMA Model with AWQ...")
    quant_config = {
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"
    }
    quantized_model = AutoAWQForCausalLM.from_pretrained(
        model_id,
        device_map='cuda' if torch.cuda.is_available() else 'cpu',
        use_auth_token=True
    )
    quantized_model.quantize(tokenizer, quant_config=quant_config, calib_data=examples)

    # Save quantized model
    print(f"\nSaving quantized model to '{quantized_model_dir}'...")
    quantized_model.save_quantized(quantized_model_dir, safetensors=True)

    # Save tokenizer files
    print(f"\nSaving tokenizer to '{quantized_model_dir}'...")
    tokenizer.save_pretrained(quantized_model_dir)

    # Verify saved files
    print("\nVerifying saved files...")
    if not verify_model_directory(quantized_model_dir):
        raise FileNotFoundError(f"Failed to save required files in '{quantized_model_dir}'")
    saved_files = os.listdir(quantized_model_dir)
    for f in ['model.safetensors', 'config.json', 'tokenizer.json']:
        if f in saved_files:
            print(f"Found: {f}")
        else:
            print(f"Missing: {f}")

    # Measure baseline memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        baseline_memory = torch.cuda.memory_allocated() / (1024 ** 2)
        print(f"\nBaseline GPU memory usage: {baseline_memory:.2f} MB")

    # Load quantized model for testing
    print("\nLoading Quantized LLaMA Model...")
    model_awq = AutoAWQForCausalLM.from_quantized(
        quantized_model_dir,
        safetensors=True,
        device_map='auto'
    )
    torch.cuda.synchronize()
    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)
    print(f"Quantized model loaded. Memory usage: {quantized_memory:.2f} MB")

    # Load original model for comparison
    print("\nLoading Original LLaMA Model...")
    model_original = AutoAWQForCausalLM.from_pretrained(model_id, use_auth_token=True, device_map='auto')
    torch.cuda.synchronize()
    original_memory = torch.cuda.memory_allocated() / (1024 ** 2)
    print(f"Original model loaded. Memory usage: {original_memory:.2f} MB")

    # Query both models
    for i, prompt in enumerate(prompts, 1):
        print(f"\nPrompt {i}: {prompt}")

        # Query original model
        print("Original Response:")
        response_original, time_original = query_model(model_original, tokenizer, prompt)
        print(f"Response: {response_original}")
        print(f"Inference Time: {time_original:.4f} seconds")

        # Query quantized model
        print("Quantized Response:")
        response_awq, time_awq = query_model(model_awq, tokenizer, prompt)
        print(f"Response: {response_awq}")
        print(f"Inference Time: {time_awq:.4f} seconds")

    print("\nQuantization and testing complete.")

except Exception as e:
    import traceback
    print(f"An error occurred: {str(e)}")
    traceback.print_exc()
    print("Please ensure all dependencies are installed, the model ID is correct, and you have a valid Hugging Face token.")

finally:
    # Clean up
    if 'quantized_model' in locals():
        del quantized_model
    if 'model_awq' in locals():
        del model_awq
    if 'model_original' in locals():
        del model_original
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()

import torch
import os
import shutil
import time
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM
from datasets import load_dataset

# Set random seed for reproducibility
torch.manual_seed(0)

# Check for GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Define model and output paths
model_id = "unsloth/Llama-3.2-1B"
quantized_model_dir = "llama-AWQ"

# Example texts for quantization calibration
def get_calibration_examples(num_examples=256):  # Increased to 256
    """Load example texts from C4 English dataset for quantization."""
    dataset = load_dataset("allenai/c4", "en", split="train", streaming=True)
    examples = []
    for i, example in enumerate(dataset):
        if i >= num_examples:
            break
        text = example['text'][:512]
        examples.append(text)
    return examples

# Define prompts to test the model
prompts = [
    "What is the capital of France, and what is its largest city?",
    "Write a short story about a robot exploring an abandoned city.",
    "If a car travels 60 miles in 1 hour, how far will it travel in 2.5 hours?",
    "Explain why the sky appears blue.",
    "What’s your favorite book, and why?"
]

def verify_model_directory(model_dir):
    """Verify that the model directory contains required files."""
    required_files = ['model.safetensors', 'config.json', 'tokenizer.json']
    return all(os.path.exists(os.path.join(model_dir, f)) for f in required_files)

def query_model(model, tokenizer, prompt, max_new_tokens=200, num_beams=10):  # Increased for better quality
    """Query the model with a prompt and return the generated response with inference time."""
    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(device)
    input_ids = inputs['input_ids']
    attention_mask = inputs['attention_mask']

    start_time = time.perf_counter()
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,
            pad_token_id=tokenizer.pad_token_id,
            early_stopping=True,
            no_repeat_ngram_size=2
        )
    inference_time = time.perf_counter() - start_time

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.strip(), inference_time

try:
    # Check if quantized_model_dir exists and remove it
    if os.path.exists(quantized_model_dir):
        print(f"\nDirectory '{quantized_model_dir}' already exists. Deleting to create a fresh quantized model...")
        shutil.rmtree(quantized_model_dir)

    # Load tokenizer
    print("\nLoading Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)
    tokenizer.pad_token = tokenizer.eos_token

    # Get calibration examples
    print("\nLoading calibration examples...")
    examples = get_calibration_examples()

    # Quantize the model using AWQ
    print("\nQuantizing LLaMA Model with AWQ...")
    quant_config = {
        "zero_point": True,
        "q_group_size": 128,
        "w_bit": 4,
        "version": "GEMM"
    }
    quantized_model = AutoAWQForCausalLM.from_pretrained(
        model_id,
        device_map='cuda' if torch.cuda.is_available() else 'cpu',
        token=True
    )
    quantized_model.quantize(tokenizer, quant_config=quant_config, calib_data=examples)

    # Save quantized model
    print(f"\nSaving quantized model to '{quantized_model_dir}'...")
    quantized_model.save_quantized(quantized_model_dir, safetensors=True)

    # Save tokenizer files
    print(f"\nSaving tokenizer to '{quantized_model_dir}'...")
    tokenizer.save_pretrained(quantized_model_dir)

    # Verify saved files
    print("\nVerifying saved files...")
    if not verify_model_directory(quantized_model_dir):
        raise FileNotFoundError(f"Failed to save required files in '{quantized_model_dir}'")
    saved_files = os.listdir(quantized_model_dir)
    for f in ['model.safetensors', 'config.json', 'tokenizer.json']:
        if f in saved_files:
            print(f"Found: {f}")
        else:
            print(f"Missing: {f}")

    # Measure baseline memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        baseline_memory = torch.cuda.memory_allocated() / (1024 ** 2)
        print(f"\nBaseline GPU memory usage: {baseline_memory:.2f} MB")

    # Load quantized model for testing
    print("\nLoading Quantized LLaMA Model...")
    model_awq = AutoAWQForCausalLM.from_quantized(
        quantized_model_dir,
        safetensors=True,
        device_map='auto'
    )
    torch.cuda.synchronize()
    quantized_memory = torch.cuda.memory_allocated() / (1024 ** 2)
    print(f"Quantized model loaded. Memory usage: {quantized_memory:.2f} MB")

    # Load original model for comparison
    print("\nLoading Original LLaMA Model...")
    model_original = AutoAWQForCausalLM.from_pretrained(model_id, token=True, device_map='auto')
    torch.cuda.synchronize()
    original_memory = torch.cuda.memory_allocated() / (1024 ** 2)
    print(f"Original model loaded. Memory usage: {original_memory:.2f} MB")

    # Query both models
    for i, prompt in enumerate(prompts, 1):
        print(f"\nPrompt {i}: {prompt}")

        # Query original model
        print("Original Response:")
        response_original, time_original = query_model(model_original, tokenizer, prompt)
        print(f"Response: {response_original}")
        print(f"Inference Time: {time_original:.4f} seconds")

        # Query quantized model
        print("Quantized Response:")
        response_awq, time_awq = query_model(model_awq, tokenizer, prompt)
        print(f"Response: {response_awq}")
        print(f"Inference Time: {time_awq:.4f} seconds")

    print("\nQuantization and testing complete.")

except Exception as e:
    import traceback
    print(f"An error occurred: {str(e)}")
    traceback.print_exc()
    print("Please ensure all dependencies are installed, the model ID is correct, and you have a valid Hugging Face token.")

finally:
    # Clean up
    if 'quantized_model' in locals():
        del quantized_model
    if 'model_awq' in locals():
        del model_awq
    if 'model_original' in locals():
        del model_original
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()

